{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download and destripe AP data from an IBL session by its PID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noah/miniforge3/envs/ceed/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-30 00:48:04.178646: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 00:48:04.369589: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-30 00:48:04.369655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-30 00:48:04.401758: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-30 00:48:04.473010: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-30 00:48:04.474201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-30 00:48:05.471385: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/noah/miniforge3/envs/ceed/lib/python3.10/site-packages/ibllib/io/raw_data_loaders.py:19: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import parse_version\n",
      "/home/noah/miniforge3/envs/ceed/lib/python3.10/site-packages/ibllib/atlas/__init__.py:202: DeprecationWarning: ibllib.atlas is deprecated. Please install iblatlas using \"pip install iblatlas\" and use this module instead\n",
      "  warnings.warn('ibllib.atlas is deprecated. Please install iblatlas using \"pip install iblatlas\" and use '\n"
     ]
    }
   ],
   "source": [
    "from analysis.data_gen_utils import download_IBL, extract_IBL, make_dataset, combine_datasets, all_units_except\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Param ALYX_LOGIN, current value is [\"intbrainlab\"]: intbrainlab\n",
      "Param HTTP_DATA_SERVER, current value is [\"https://ibl.flatironinstitute.org/public\"]: \n",
      "Param HTTP_DATA_SERVER_LOGIN, current value is [\"None\"]: \n",
      "Enter the FlatIron HTTP password for None (leave empty to keep current):  ········\n",
      "Enter the location of the download cache, current value is [\"/home/noah/Downloads/ONE/openalyx.internationalbrainlab.org\"]: \n",
      "Would you like to set this URL as the default one? [Y/n] \n",
      "Are the above settings correct? [Y/n] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE Parameter files location: /home/noah/.one\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Alyx password for \"intbrainlab\": ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:one.api:HTTPError: Failed to load the remote cache file\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m t_window \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1000\u001b[39m] \u001b[38;5;66;03m#in seconds\u001b[39;00m\n\u001b[1;32m     10\u001b[0m overwrite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m rec1, meta_file_sess1 \u001b[38;5;241m=\u001b[39m \u001b[43mdownload_IBL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpid_sess1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_folder_sess1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m rec2, meta_file_sess2 \u001b[38;5;241m=\u001b[39m download_IBL(pid\u001b[38;5;241m=\u001b[39mpid_sess2, t_window\u001b[38;5;241m=\u001b[39mt_window, save_folder\u001b[38;5;241m=\u001b[39msave_folder_sess2, overwrite\u001b[38;5;241m=\u001b[39moverwrite)\n",
      "File \u001b[0;32m~/code/CEED/analysis/data_gen_utils.py:490\u001b[0m, in \u001b[0;36mdownload_IBL\u001b[0;34m(pid, save_folder, cache_folder, t_window, overwrite, do_spikeinterface_destripe)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract and format data from a specific IBL session.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;124;03m    absolute path location of corresponding metadata file for IBL session\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m one \u001b[38;5;241m=\u001b[39m  ONE(base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://openalyx.internationalbrainlab.org\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 490\u001b[0m eid, probe \u001b[38;5;241m=\u001b[39m \u001b[43mone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid2eid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m band \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124map\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# either 'ap' or 'lf'\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;66;03m# Use IBL streamer to download the data\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/util.py:161\u001b[0m, in \u001b[0;36mrefresh.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh_cache(mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/api.py:1869\u001b[0m, in \u001b[0;36mOneAlyx.pid2eid\u001b[0;34m(self, pid, query_type)\u001b[0m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsertions\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1868\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConverting probe IDs required remote connection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1869\u001b[0m rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malyx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minsertions\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession\u001b[39m\u001b[38;5;124m'\u001b[39m], rec[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:1001\u001b[0m, in \u001b[0;36mAlyxClient.rest\u001b[0;34m(self, url, action, id, data, files, no_cache, **kwargs)\u001b[0m\n\u001b[1;32m    999\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^/*[^?/]*\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;66;03m# make sure the queried endpoint exists, if not throw an informative error\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m endpoint \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_schemes\u001b[49m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1002\u001b[0m     av \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_schemes\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m k\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m k]\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mREST endpoint \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m endpoint \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist. Available \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m   1004\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mendpoints are \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m       \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m       \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(av))\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:518\u001b[0m, in \u001b[0;36mAlyxClient.rest_schemes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;66;03m# Delayed fetch of rest schemes speeds up instantiation\u001b[39;00m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_schemes:\n\u001b[0;32m--> 518\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_schemes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/docs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpires\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweeks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rest_schemes\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:876\u001b[0m, in \u001b[0;36mAlyxClient.get\u001b[0;34m(self, rest_query, **kwargs)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, rest_query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    859\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m    Sends a GET request to the Alyx server. Will raise an exception on any status_code\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m    other than 200, 201.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;124;03m    JSON interpreted dictionary from response.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 876\u001b[0m     rep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generic_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrest_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rep, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(rep\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprevious\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    878\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m<\u001b[39m rep[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:123\u001b[0m, in \u001b[0;36m_cache_response.<locals>.wrapper_decorator\u001b[0;34m(alyx_client, expires, clobber, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m cached\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43malyx_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mConnectionError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clobber:\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:584\u001b[0m, in \u001b[0;36mAlyxClient._generic_request\u001b[0;34m(self, reqfunction, rest_query, data, files)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;129m@_cache_response\u001b[39m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generic_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, reqfunction, rest_query, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, files\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_logged_in:\n\u001b[0;32m--> 584\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# makes sure the base url is the one from the instance\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     rest_query \u001b[38;5;241m=\u001b[39m rest_query\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/one/webclient.py:657\u001b[0m, in \u001b[0;36mAlyxClient.authenticate\u001b[0;34m(self, username, password, cache_token, force)\u001b[0m\n\u001b[1;32m    655\u001b[0m     password \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_par, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mALYX_PWD\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m password \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilent:\n\u001b[0;32m--> 657\u001b[0m     password \u001b[38;5;241m=\u001b[39m \u001b[43mgetpass\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnter Alyx password for \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43musername\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    659\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m'\u001b[39m: username, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m'\u001b[39m: password}\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/ipykernel/kernelbase.py:1244\u001b[0m, in \u001b[0;36mKernel.getpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `stream` parameter of `getpass.getpass` will have no effect when using ipykernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1242\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[0;32m-> 1244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ceed/lib/python3.10/site-packages/ipykernel/kernelbase.py:1304\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1302\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "pid_sess1 = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "pid_sess2 = 'febb430e-2d50-4f83-87a0-b5ffbb9a4943'\n",
    "\n",
    "save_dir = '/mnt/data/noah/IBL_data_CEED/'\n",
    "\n",
    "save_folder_sess1 = save_dir + pid_sess1\n",
    "save_folder_sess2 = save_dir + pid_sess2\n",
    "\n",
    "t_window = [0, 1000] #in seconds\n",
    "overwrite = False\n",
    "rec1, meta_file_sess1 = download_IBL(pid=pid_sess1, t_window=t_window, save_folder=save_folder_sess1, overwrite=overwrite)\n",
    "rec2, meta_file_sess2 = download_IBL(pid=pid_sess2, t_window=t_window, save_folder=save_folder_sess2, overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''extract the all data needed to make CEED dataset\n",
    "spike_idx_sess: spike_times, channels, neurons (if use_labels=True)\n",
    "geom_sess: channels x 2\n",
    "chan_idx_sess: waveform extraction channels for each channel\n",
    "templates_sess: templates across all channels for all neurons\n",
    "'''\n",
    "recompute = False\n",
    "\n",
    "if recompute:\n",
    "    spike_idx_sess1, geom_sess1, chan_idx_sess1, templates_sess1 = extract_IBL(rec=rec1, \n",
    "                                                                               meta_fp=meta_file_sess1, \n",
    "                                                                               pid=pid_sess1, \n",
    "                                                                               t_window=t_window, \n",
    "                                                                               use_labels=True)\n",
    "    spike_idx_sess2, geom_sess2, chan_idx_sess2, templates_sess2 = extract_IBL(rec=rec2, \n",
    "                                                                               meta_fp=meta_file_sess2, \n",
    "                                                                               pid=pid_sess2, \n",
    "                                                                               t_window=t_window,\n",
    "                                                                               use_labels=True)\n",
    "    np.save('spike_idx_sess1.npy', spike_idx_sess1)\n",
    "    np.save('geom_sess1.npy', geom_sess1)\n",
    "    np.save('chan_idx_sess1.npy', chan_idx_sess1)\n",
    "    np.save('templates_sess1.npy', templates_sess1)\n",
    "    np.save('spike_idx_sess2.npy', spike_idx_sess2)\n",
    "    np.save('geom_sess2.npy', geom_sess2)\n",
    "    np.save('chan_idx_sess2.npy', chan_idx_sess2)\n",
    "    np.save('templates_sess2.npy', templates_sess2)\n",
    "else:\n",
    "    spike_idx_sess1 = np.load('spike_idx_sess1.npy')\n",
    "    geom_sess1 = np.load('geom_sess1.npy')\n",
    "    chan_idx_sess1 = np.load('chan_idx_sess1.npy')\n",
    "    templates_sess1 = np.load('templates_sess1.npy')\n",
    "    spike_idx_sess2 = np.load('spike_idx_sess2.npy')\n",
    "    geom_sess2 = np.load('geom_sess2.npy')\n",
    "    chan_idx_sess2 = np.load('chan_idx_sess2.npy')\n",
    "    templates_sess2 = np.load('templates_sess2.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Unit Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a denoised template on the 40 extracted channels (not all 40 used to train CEED).\n",
    "template_id = 454\n",
    "template = templates_sess1[template_id]\n",
    "max_channel = np.argmax(np.ptp(template,0))\n",
    "plt.plot(templates_sess1[template_id][:,chan_idx_sess1[max_channel]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a couple training datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session 1 units to get data from and dataset save path\n",
    "# can look at templates of neural units to gauge whether to extract spikes from them for a CEED dataset\n",
    "selected_units_sess1 = np.arange(400)\n",
    "dataset_folder_sess1 = save_folder_sess1 + '/ds'\n",
    "\n",
    "# make a dataset for training\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "inference = False\n",
    "train_num = 400\n",
    "val_num = 0\n",
    "test_num = 100\n",
    "save_covs = True\n",
    "num_chans_extract = 21\n",
    "normalize = False #True for cell-type dataset\n",
    "shift = False\n",
    "save_fewer = False\n",
    "train_set1, val_set1, test_set1, train_geom_locs1, val_geom_locs1, \\\n",
    "test_geom_locs1, train_max_chan1, val_max_chan1, test_max_chan1 = make_dataset(rec=rec1, \n",
    "                                                                               spike_index=spike_idx_sess1,\n",
    "                                                                               geom=geom_sess1, \n",
    "                                                                               save_path=dataset_folder_sess1, \n",
    "                                                                               chan_index=chan_idx_sess1, \n",
    "                                                                               templates=templates_sess1, \n",
    "                                                                               unit_ids=selected_units_sess1, \n",
    "                                                                               train_num=train_num, \n",
    "                                                                               val_num=val_num, \n",
    "                                                                               test_num=test_num, \n",
    "                                                                               save_covs=save_covs, \n",
    "                                                                               num_chans_extract=num_chans_extract, \n",
    "                                                                               normalize=normalize, \n",
    "                                                                               shift=shift, \n",
    "                                                                               inference=inference,\n",
    "                                                                               save_fewer=save_fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose for session 2 we want a larger dataset comprised of most of the units in the recording, \n",
    "# but excluding some units that might have poor spikes in the recording or which kilosort did a poor job of recognizing. \n",
    "# then we can save out all of the units in the recording to the dataset and plot some spikes from the units (with plot=True)\n",
    "# and then we can go through and look at which units look like they have good, useful spikes for training. for this we'll\n",
    "# save out all units at first by specifying no unit ids.\n",
    "selected_units_sess2 = None\n",
    "dataset_folder_sess2 = save_folder_sess2 + '/ds'\n",
    "\n",
    "# make a dataset for training\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "inference = False\n",
    "train_num = 400\n",
    "val_num = 0\n",
    "test_num = 100\n",
    "save_covs = True\n",
    "num_chans_extract = 21\n",
    "normalize = False\n",
    "shift = False\n",
    "\n",
    "train_set2, val_set2, test_set2, train_geom_locs, val_geom_locs2, \\\n",
    "test_geom_locs2, train_max_chan2, val_max_chan2, test_max_chan2 = make_dataset(rec=rec2, \n",
    "                                                                               spike_index=spike_idx_sess2,\n",
    "                                                                               geom=geom_sess2, \n",
    "                                                                               save_path=dataset_folder_sess2, \n",
    "                                                                               chan_index=chan_idx_sess2, \n",
    "                                                                               templates=templates_sess2, \n",
    "                                                                               unit_ids=selected_units_sess2, \n",
    "                                                                               train_num=train_num, \n",
    "                                                                               val_num=val_num, \n",
    "                                                                               test_num=test_num, \n",
    "                                                                               save_covs=save_covs, \n",
    "                                                                               num_chans_extract=num_chans_extract, \n",
    "                                                                               plot=True,\n",
    "                                                                               normalize=normalize, \n",
    "                                                                               shift=shift, \n",
    "                                                                               inference=inference,\n",
    "                                                                               save_fewer=save_fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after looking at all of the plotted units in session 2's dataset folder (under wf_plots) we decide to exclude certain units.\n",
    "# we can also decide to make selected_units_sess2 out of the units we decide are good rather than exclude units \n",
    "# if we find too many bad units (we would just define it as a list of units in this latter case).\n",
    "bad_units_sess2 = [130, 250, 340]\n",
    "selected_units_sess2 = all_units_except(spike_index=spike_idx_sess2, exclude_units=bad_units_sess2)\n",
    "\n",
    "# now we can create the same dataset, but overwrite the previous files with the new ones that exclude the unit ids we don't want\n",
    "train_set2, val_set2, test_set2, train_geom_locs, val_geom_locs2, \\\n",
    "test_geom_locs2, train_max_chan2, val_max_chan2, test_max_chan2 = make_dataset(rec=rec2, \n",
    "                                                                               spike_index=spike_idx_sess2,\n",
    "                                                                               geom=geom_sess2, \n",
    "                                                                               save_path=dataset_folder_sess2, \n",
    "                                                                               chan_index=chan_idx_sess2, \n",
    "                                                                               templates=templates_sess2, \n",
    "                                                                               unit_ids=selected_units_sess2, \n",
    "                                                                               train_num=train_num, \n",
    "                                                                               val_num=val_num, \n",
    "                                                                               test_num=test_num, \n",
    "                                                                               save_covs=save_covs, \n",
    "                                                                               num_chans_extract=num_chans_extract, \n",
    "                                                                               plot=False,\n",
    "                                                                               normalize=normalize, \n",
    "                                                                               shift=shift, \n",
    "                                                                               inference=inference,\n",
    "                                                                               save_fewer=save_fewer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds_path = '/media/cat/data/IBL_data_CEED/combined'\n",
    "\n",
    "# combine the two training datasets into a larger one for more unit diversity\n",
    "dataset_list = [dataset_folder_sess1, dataset_folder_sess2]\n",
    "combine_datasets(dataset_list, combined_ds_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an inference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session 1 units to get data from and dataset save path\n",
    "selected_units_inference = np.arange(400)\n",
    "inference_ds_path = save_folder_sess1 + '/ds_inference'\n",
    "\n",
    "# make a dataset for inference (testing)\n",
    "# will create a folder with the spike, probe channel number, and corresponding channel location datasets in the train, val, test splits\n",
    "# optionally also saves out spatial and temporal noise covariance matrices\n",
    "inference = True\n",
    "test_num = 500\n",
    "save_covs = True\n",
    "num_chans_extract = 21\n",
    "normalize = False\n",
    "shift = False\n",
    "\n",
    "test_set, test_geom_locs, test_max_chan = make_dataset(rec=rec1, \n",
    "                                                       spike_index=spike_idx_sess1,\n",
    "                                                       geom=geom_sess1, \n",
    "                                                       save_path=inference_ds_path, \n",
    "                                                       chan_index=chan_idx_sess1, \n",
    "                                                       templates=templates_sess1, \n",
    "                                                       unit_ids=selected_units_inference, \n",
    "                                                       train_num=train_num, \n",
    "                                                       val_num=val_num, \n",
    "                                                       test_num=test_num, \n",
    "                                                       save_covs=save_covs, \n",
    "                                                       num_chans_extract=num_chans_extract, \n",
    "                                                       normalize=normalize, \n",
    "                                                       shift=shift, \n",
    "                                                       inference=inference,\n",
    "                                                       save_fewer=save_fewer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ceed]",
   "language": "python",
   "name": "conda-env-ceed-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
